Hadoop Production Issues 

1. DataNode Failures

Description: DataNodes may fail due to hardware or network issues, leading to data loss or unavailability.

Possible Causes:

Disk failures

Network issues

JVM crashes


Solutions:

Check disk health and ensure sufficient storage.

Investigate network connectivity.

Configure DataNode log monitoring and set up alerting.

Enable DataNode automatic restart in Hadoop configuration (dfs.datanode.restart.retries).



2. NameNode Overload

Description: NameNode becomes overloaded due to excessive metadata handling or client requests, causing delays or failures in accessing HDFS.

Possible Causes:

Large number of small files

Memory pressure

Too many client requests


Solutions:

Use HDFS Federation to distribute metadata load.

Tune NameNode heap size and garbage collection settings.

Consolidate small files using sequence files or HAR (Hadoop Archive).

Use Secondary NameNode or Checkpoints effectively.



3. Cluster Resource Contention

Description: Resource contention can lead to job failures or significant delays.

Possible Causes:

Multiple resource-intensive jobs running simultaneously.

Insufficient CPU or memory allocation.


Solutions:

Use CapacityScheduler or FairScheduler for resource allocation.

Tune job settings to limit resource usage (mapreduce.memory.mb, yarn.nodemanager.resource.memory-mb).

Monitor cluster resource usage via Ambari or Cloudera Manager.


1. Hadoop Production Issues

1.1. DataNode Failures

Description: DataNodes may fail due to hardware or network issues, leading to data loss or unavailability.

Possible Causes:

Disk failures

Network connectivity problems

JVM crashes


Solutions:

Regularly monitor disk health and replace faulty disks promptly.

Ensure robust network infrastructure and redundancy.

Configure DataNode log monitoring and set up automated alerting.

Enable automatic restart for DataNodes in Hadoop configurations (dfs.datanode.restart.retries).



1.2. NameNode Overload

Description: NameNode becomes overloaded due to excessive metadata handling or client requests, causing delays or failures in accessing HDFS.

Possible Causes:

Large number of small files

Insufficient NameNode memory

High number of concurrent client requests


Solutions:

Implement HDFS Federation to distribute metadata across multiple NameNodes.

Increase NameNode heap size and optimize garbage collection settings.

Consolidate small files using sequence files or Hadoop Archives (HAR).

Limit the number of concurrent client connections.



1.3. Cluster Resource Contention

Description: Resource contention can lead to job failures or significant delays.

Possible Causes:

Multiple resource-intensive jobs running simultaneously.

Inadequate CPU or memory allocation per node.


Solutions:

Utilize CapacityScheduler or FairScheduler for effective resource allocation.

Tune YARN configurations to allocate appropriate resources (mapreduce.memory.mb, yarn.nodemanager.resource.memory-mb).

Monitor cluster resource usage with tools like Ambari or Cloudera Manager and adjust workloads accordingly.



1.4. HDFS Corruption

Description: Data corruption within HDFS can lead to inaccessible or unreliable data.

Possible Causes:

Hardware failures

Software bugs

Improper shutdowns


Solutions:

Regularly run HDFS integrity checks using tools like fsck.

Implement redundant storage policies (e.g., replication factor).

Ensure clean shutdown procedures to prevent abrupt terminations.



1.5. Slow Data Processing

Description: Data processing tasks run slower than expected, impacting overall performance.

Possible Causes:

Inefficient MapReduce jobs

Poor data locality

Inadequate cluster resources


Solutions:

Optimize MapReduce job configurations and logic.

Ensure data is stored close to where processing occurs to maximize data locality.

Scale the cluster horizontally to provide additional resources.



1.6. Zookeeper Coordination Issues

Description: Problems with Zookeeper can disrupt coordination services within the Hadoop ecosystem.

Possible Causes:

Network partitions

Zookeeper ensemble instability

Configuration mismatches


Solutions:

Ensure high availability by maintaining an odd number of Zookeeper nodes.

Monitor Zookeeper health and network stability.

Verify and standardize Zookeeper configurations across the cluster.



1.7. Insufficient Monitoring and Alerting

Description: Lack of proper monitoring can delay the detection of critical issues.

Possible Causes:

Inadequate monitoring tools

Missing key performance indicators (KPIs)

Poor alert configuration


Solutions:

Implement comprehensive monitoring solutions like Prometheus, Grafana, or Ambari.

Define and track essential KPIs for cluster health and performance.

Configure alerts for critical thresholds and anomalies.



1.8. Security Vulnerabilities

Description: Security breaches can lead to unauthorized data access or data loss.

Possible Causes:

Misconfigured access controls

Unpatched software vulnerabilities

Weak authentication mechanisms


Solutions:

Implement robust security policies and access controls using Kerberos.

Regularly update and patch Hadoop components.

Enforce strong authentication and encryption for data in transit and at rest.



1.9. Version Incompatibilities

Description: Incompatibilities between Hadoop components can cause system instability.

Possible Causes:

Mixing incompatible versions of Hadoop, Hive, or other ecosystem tools.

Upgrading components without ensuring compatibility.


Solutions:

Maintain a consistent versioning strategy across all Hadoop components.

Consult compatibility matrices before performing upgrades.

Test upgrades in a staging environment before production deployment.



1.10. Backup and Recovery Failures

Description: Inadequate backup strategies can lead to data loss in case of failures.

Possible Causes:

Infrequent or incomplete backups

Corrupted backup data

Lack of a clear recovery plan


Solutions:

Implement regular and comprehensive backup schedules.

Validate backup integrity periodically.

Develop and document a robust disaster recovery plan.

Known Issues in HDFS
Learn about the known issues in HDFS, the impact or changes to the functionality, and the workaround.

CDPD-67230: Rolling restart can cause failed writes on small clusters
In a rolling restart, if the cluster has less than 10 datanodes existing writers can fail with an error indicating a new block cannot be allocated and all nodes are excluded. This is because the client has attempted to use all the datanodes in the cluster, and failed to write to each of them as they were restarted. This will only happen on small clusters of less than 10 datanodes, as larger clusters have more spare node to allow the write to continue.
None.
CDPD-60873: java.io.IOException: Got error, status=ERROR, status message, ack with firstBadLink while fixing the HDFS corrupt file during rollback.
Increase the value of dfs.client.block.write.retries to the number of nodes in the cluster and perform Deploy client configuration procedure for rectification.
CDPD-60431: Configuration difference between 7.1.7 SP2 and 7.1.9.0 results
Component

Configuration	Old Value	New Value	Description
HDFS	dfs.permissions.ContentSummary.subAccess	Not set	True	Performance optimization for NN content summary API
HDFS	dfs.datanode.handler.count	3	10	Optimal value for DN server threads on large clusters
None
CDPD-60387: Configuration difference between 7.1.8.3 and 7.1.9.0 results
Component

Configuration	Old Value	New Value	Description
HDFS	dfs.namenode.accesstime.precision	Not set	0	Optimal value for NN performance on large clusters
HDFS	dfs.datanode.handler.count	3	10	Optimal value for DN server threads on large clusters
None
OPSAPS-64307: When the JournalNodes on a cluster are restarted, the Add new NameNode wizard for HDFS service might fail to bootstrap the new NameNode. If there was no new fsImage created from the time JournalNodes restarted, during the restarti, the edit logs were rolled in the system.
If the bootstrap fails during the Add new NameNode wizard, then perform the following steps:
Delete the newly added NameNode and FailoverController
Move the active HDFS NameNode to safe mode
Perform the Save Namespace operation on the active HDFS NameNode
Leave safe mode on the active HDFS NameNode
Try to add the new NameNode again
note
Note that entering safe mode will disable writes to HDFS which causes a service disruption. If you cannot enter the safe mode, delete the newly added NameNode and FailoverController in the HDFS service and wait until HDFS automatically creates a new fsImage and then try adding the new NameNode again with the wizard.
OPSAPS-64363: Deleting of additional Standby Namenode does not delete the ZKFC role and this has to be done manually.
None
CDPD-28390: Rolling restart of the HDFS JournalNodes may time out on Ubuntu20.
If the restart operation times out, you can manually stop and restart the Name Node and Journal Node services one by one.
OPSAPS-60832: When decommission of DN runs for a longer time and when decommission monitor's kerberos ticket expires, it is not auto-renewed. Decommission of DN is not completed in Cloudera Manager as decommission monitor fails to fetch the state of DN after kerberos ticket expiry.
Decommission state of DN can be fetched using CLI command hdfs dfsadmin -report.
OPSAPS-55788: WebHDFS is always enabled. The Enable WebHDFS checkbox does not take effect.
None.
OPSAPS-63299: Disable HA command for a nameservice does not work if the nameservice has more than 2 NNs defined.
None
OPSAPS-63301: Deleting nameservice command does not delete all the NNs belonging to the nameservice, if there are more than two NNs that are assigned to the nameservice.
None
Unsupported Features
The following HDFS features are currently not supported in Cloudera Data Platform:
ACLs for the NFS gateway (HADOOP-11004)
Aliyun Cloud Connector (HADOOP-12756)
Allow HDFS block replicas to be provided by an external storage system (HDFS-9806)
Consistent standby Serving reads (HDFS-12943)
Cost-Based RPC FairCallQueue (HDFS-14403)
HDFS Router Based Federation (HDFS-10467)
NameNode Federation (HDFS-1052)
NameNode Port-based Selective Encryption (HDFS-13541)
Non-Volatile Storage Class Memory (SCM) in HDFS Cache Directives (HDFS-13762)
OpenStack Swift (HADOOP-8545)
SFTP FileSystem (HADOOP-5732)
Storage policy satisfier (HDFS-10285)
Technical Service Bulletins
TSB 2022-549: Possible HDFS Erasure Coded (EC) data loss when EC blocks are over-replicated
Cloudera has detected a bug that can cause loss of data that is stored in HDFS Erasure Coded (EC) files in an unlikely scenario.
Some EC blocks may be inadvertently deleted due to a bug in how the NameNode chooses excess or over-replicated block replicas for deletion. One possible cause of over-replication is running the HDFS balancer soon after a NameNode goes into failover mode.
In a rare situation, the redundant blocks could be placed in such a way that one replica is in one rack, and few redundant replicas are in the same rack. Such placement causes a counting bug (HDFS-16420) to be triggered. Instead of deleting just the redundant replicas, the original replica may also be deleted.
Usually this is not an issue, because the lost replica can be detected and reconstructed from the remaining data and parity blocks. However, if multiple blocks in an EC Block Group are affected by this counting bug within a short time, the block cannot be reconstructed anymore. For example, 4 blocks are affected out of 9 for the RS(6,3) policy.
Another situation is recommissioning multiple nodes back into the same rack of the cluster where the current live replica exists.
Upstream JIRA
HDFS-16420
Knowledge article
For the latest update on this issue see the corresponding Knowledge article: TSB 2022-549: Possible HDFS Erasure Coded (EC) data loss when EC blocks are over-replicated


OPSAPS-61188: Zookeeper start fails with custom user as contents inside /var/lib/zookeeper have "zookeeper" as owner instead of the custom user
In Cloudera Manager the Process Username for ZooKeeper can be changed from the default zookeeper value to any custom value. This configuration change in Cloudera Manager automatically changes the owner of the var/lib/zookeeper folder but keeps zookeeper as the owner of any folders or files inside var/lib/zookeeper, such as myid and version-2. As a result ZooKeeper fails to start because it needs to read the snapshots and txnlogs from the var/lib/zookeeper/version-2 folder when starting.
Ensure that you changed the Process Username to a username that exists on the OS.
Manually change the owner.
Log in to the node.
Recursively change the owner of var/lib/zookeeper using the chown -R command.
Zookeeper-client does not use ZooKeeper TLS/SSL automatically
The command-line tool ‘zookeeper-client’ is installed to all Cloudera Nodes and it can be used to start the default Java command line ZooKeeper client. However even when ZooKeeper TLS/SSL is enabled, the zookeeper-client command connects to localhost:2181, without using TLS/SSL.
Manually configure the 2182 port, when zookeeper-client connects to a ZooKeeper cluster.The following is an example of connecting to a specific three-node ZooKeeper cluster using TLS/SSL:
CLIENT_JVMFLAGS="-Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.ssl.keyStore.location=<path to your configured keystore> -Dzookeeper.ssl.keyStore.password=<the password you configured for the keystore>  -Dzookeeper.ssl.trustStore.location=<path to your configured truststore> -Dzookeeper.ssl.trustStore.password=<the password you configured for the truststore> -Dzookeeper.client.secure=true" zookeeper-client -server <your.zookeeper.server-1>:2182,<your.zookeeper.server-2>:2182,<your.zookeeper.server-3>:2182
